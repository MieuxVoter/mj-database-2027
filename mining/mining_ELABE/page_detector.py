# coding: utf-8
"""
D√©tecteur de pages de donn√©es dans les PDFs ELABE.
"""

import pathlib
from typing import List, Tuple, Optional
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer


class PageDetector:
    """D√©tecte les pages contenant des donn√©es de sondage."""
    
    def __init__(self, pdf_path: pathlib.Path):
        """
        Initialise le d√©tecteur.
        
        Args:
            pdf_path: Chemin vers le PDF √† analyser
        """
        self.pdf_path = pdf_path
    
    def detect_data_pages(self, start_page: int = 1, end_page: int = 30) -> List[Tuple[int, str]]:
        """
        D√©tecte les pages contenant des tableaux de donn√©es.
        
        Strat√©gie : Une page de donn√©es ELABE contient :
        - Un titre comme "Le classement des personnalit√©s"
        - Une mention de population (Ensemble, abstentionnistes, etc.)
        - Un bloc avec 20-35 noms de candidats
        - Des colonnes de scores num√©riques
        
        Args:
            start_page: Page de d√©but (incluse)
            end_page: Page de fin (incluse)
        
        Returns:
            Liste de tuples (num√©ro_page, type_population)
        """
        data_pages = []
        
        for page_num in range(start_page, end_page + 1):
            result = self._check_page(page_num)
            if result:
                data_pages.append(result)
        
        return data_pages
    
    def _check_page(self, page_num: int) -> Optional[Tuple[int, str]]:
        """
        V√©rifie si une page contient des donn√©es.
        
        Args:
            page_num: Num√©ro de page √† v√©rifier
        
        Returns:
            Tuple (page_num, population) si c'est une page de donn√©es, None sinon
        """
        for page_number, page_layout in enumerate(extract_pages(str(self.pdf_path)), start=1):
            if page_number != page_num:
                continue
            
            # Extraire tout le texte de la page
            page_text = ""
            text_blocks = []
            
            for element in page_layout:
                if isinstance(element, LTTextContainer):
                    text = element.get_text().strip()
                    if text:
                        page_text += text + "\n"
                        
                        lines = [l.strip() for l in text.split('\n') if l.strip()]
                        if len(lines) >= 20:
                            text_blocks.append({
                                'text': text,
                                'lines': lines,
                                'line_count': len(lines)
                            })
            
            # V√©rifier les marqueurs d'une page de donn√©es
            has_title = "Le classement des personnalit√©s" in page_text
            has_candidates = any(block['line_count'] >= 20 for block in text_blocks)
            
            if not (has_title and has_candidates):
                break
            
            # D√©terminer le type de population
            population = self._identify_population(page_text)
            
            return (page_num, population)
        
        return None
    
    def _identify_population(self, page_text: str) -> str:
        """
        Identifie le type de population depuis le texte de la page.
        
        Args:
            page_text: Texte complet de la page
        
        Returns:
            Identifiant de population ("all", "absentionists", etc.)
        """
        # Normaliser les apostrophes typographiques (U+2019) en apostrophes standard (U+0027)
        text_lower = page_text.lower().replace('\u2019', "'")
        
        # Patterns de d√©tection (avec apostrophes standard)
        patterns = {
            "all": [
                "ensemble des fran√ßais",
                "tous les fran√ßais",
                "l'ensemble des fran√ßais"
            ],
            "absentionists": [
                "abstentionnistes",
                "votes blancs et nuls",
                "non-inscrits"
            ],
            "macron": [
                "√©lecteurs d'emmanuel macron",
                "√©lecteurs de macron"
            ],
            "left": [
                "√©lecteurs de gauche et des √©cologistes",
                "√©lecteurs de gauche",
                "sympathisants de gauche"
            ],
            "farright": [
                "√©lecteurs de marine le pen et d'√©ric zemmour",
                "√©lecteurs de marine le pen",
                "√©lecteurs d'extr√™me droite",
                "√©lecteurs du rassemblement national"
            ]
        }
        
        # Chercher le premier pattern qui matche
        for pop_id, keywords in patterns.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return pop_id
        
        # Si rien ne matche, retourner "unknown"
        return "unknown"
    
    def get_summary(self, data_pages: List[Tuple[int, str]]) -> str:
        """
        G√©n√®re un r√©sum√© des pages d√©tect√©es.
        
        Args:
            data_pages: Liste des pages d√©tect√©es
        
        Returns:
            R√©sum√© format√©
        """
        if not data_pages:
            return "Aucune page de donn√©es d√©tect√©e"
        
        summary = f"üìä {len(data_pages)} page(s) de donn√©es d√©tect√©e(s) :\n\n"
        
        for page_num, population in data_pages:
            pop_name = {
                "all": "Ensemble des Fran√ßais",
                "absentionists": "Abstentionnistes",
                "macron": "√âlecteurs de Macron",
                "left": "√âlecteurs de gauche",
                "farright": "√âlecteurs d'extr√™me droite",
                "unknown": "Population inconnue"
            }.get(population, population)
            
            summary += f"  ‚Ä¢ Page {page_num:2d}: {pop_name}\n"
        
        return summary
