# ğŸ§© BasePipeline â€” Module de base pour pipelines dâ€™extraction depuis PDF

## ğŸ“˜ PrÃ©sentation

`base_pipeline.py` dÃ©finit la classe abstraite **`BasePipeline`**, qui constitue le **socle commun** de tous les pipelines dâ€™extraction et de construction de donnÃ©es du projet **Cluster17 Mining**.  

Cette classe Ã©tablit un **modÃ¨le dâ€™orchestration gÃ©nÃ©rique**, garantissant :
- la validation des paramÃ¨tres dâ€™entrÃ©e,  
- le nettoyage automatique des anciens fichiers,  
- la gestion uniforme du cycle dâ€™extraction et de construction,  
- la journalisation dÃ©taillÃ©e des opÃ©rations.

Elle implÃ©mente le **pattern â€œTemplate Methodâ€**, oÃ¹ la structure du flux est fixÃ©e mais les Ã©tapes concrÃ¨tes (`extract`, `build`) sont dÃ©finies par les sous-classes.

---

## âš™ï¸ Objectif

`BasePipeline` fournit un cadre robuste et extensible permettant :

- dâ€™exÃ©cuter un pipeline complet sans rÃ©pÃ©ter la logique dâ€™orchestration ;
- de garantir la cohÃ©rence des logs et la gestion des erreurs ;
- de simplifier la crÃ©ation de pipelines spÃ©cifiques (ex. `Cluster17Pipeline`).

---

## ğŸ§  Structure de la classe

```python
class BasePipeline(ABC):
    def __init__(self, pdf_path: Path, poll_id: str):
        ...
    @abstractmethod
    def extract(self) -> List[Dict[str, Any]]:
        ...
    @abstractmethod
    def build(self, extracted_data) -> int:
        ...
    def run(self):
        ...
````

---

## ğŸ” Cycle dâ€™exÃ©cution

| Ã‰tape | Description                                   | MÃ©thode correspondante      |
| ----- | --------------------------------------------- | --------------------------- |
| 1     | Validation du fichier PDF et de lâ€™identifiant | `_validate_inputs()`        |
| 2     | Valide du fichier metadata.txt.               | `_validate_metadata()`      |
| 3     | Suppression des anciens fichiers CSV/TXT      | `_cleanup_existing_files()` |
| 4     | Extraction des donnÃ©es brutes                 | `extract()` *(abstraite)*   |
| 5     | Construction des artefacts finaux             | `build()` *(abstraite)*     |
| 6     | Journalisation du rÃ©sultat global             | `run()`                     |

---

## ğŸ§© Utilisation

### 1. CrÃ©er une sous-classe

```python
from pathlib import Path
from mining.base_pipeline import BasePipeline

class MyPipeline(BasePipeline):
    def extract(self):
        print("Extraction en cours...")
        return [{"df": "mock_dataframe"}]

    def build(self, extracted_data):
        print(f"Construction terminÃ©e ({len(extracted_data)} tables traitÃ©es)")
        return len(extracted_data)
```

---

### 2. ExÃ©cuter le pipeline

```python
if __name__ == "__main__":
    pdf_file = Path("data/cluster17_202511.pdf")
    pipeline = MyPipeline(pdf_file, poll_id="cluster17_202511")
    pipeline.run()
```

**Sortie attendue :**

```
ğŸ“„ Validation du fichier << metadata.txt >>..
ğŸ§¹ Nettoyage des anciens fichiers avant traitement...
ğŸ” DÃ©tection et extraction des pages de donnÃ©es...
ğŸ“¦ Extraction et construction des CSV...
âœ… 1 fichier(s) CSV gÃ©nÃ©rÃ©(s)
```

---

## ğŸ§¾ DÃ©tails des mÃ©thodes

### `__init__(self, pdf_path, poll_id)`

Initialise le pipeline et valide les entrÃ©es :

* `pdf_path` : chemin vers le fichier source (PDF, API, etc.)
* `poll_id` : identifiant du sondage (ex. `"cluster17_202511"`)

---

### `_validate_metadata()`  

VÃ©rifie :
* que `metadata.txt` est un `Path` existant,
* que `metadata.txt` a une structure minimale,

### `_validate_inputs()`

VÃ©rifie :

* que `pdf_path` est un `Path` existant,
* que `poll_id` est une chaÃ®ne valide.
  LÃ¨ve `TypeError` ou `FileNotFoundError` en cas dâ€™erreur.

---

### `_cleanup_existing_files(extensions=("csv", "txt"))`

Supprime les anciens fichiers `.csv` et `.txt` dans le rÃ©pertoire du PDF avant un nouveau traitement.
Les fichiers inaccessibles sont ignorÃ©s mais journalisÃ©s.

---

### `extract()`

MÃ©thode **abstraite** Ã  implÃ©menter dans les sous-classes.
Doit renvoyer une **liste de dictionnaires** dÃ©crivant les tableaux extraits, par exemple :

```python
[
  {"Page": 1, "Population": "GÃ©nÃ©rale", "df": <DataFrame>},
  {"Page": 2, "Population": "Jeunes", "df": <DataFrame>}
]
```

---

### `build(extracted_data)`

MÃ©thode **abstraite** chargÃ©e de construire les fichiers finaux (CSV, TXT, etc.).
Retourne le **nombre de fichiers crÃ©Ã©s**.

---

### `run()`

MÃ©thode principale orchestrant le processus complet :

1. Nettoyage des anciens fichiers
2. Extraction des donnÃ©es
3. Construction des artefacts
4. Journalisation du rÃ©sultat

---

## ğŸ§© Exemple concret â€” `Cluster17Pipeline`

ImplÃ©mentation pratique dans `mining_CLUSTER17/orchestrator.py` :

```python
class Cluster17Pipeline(BasePipeline):
    def extract(self):
        extractor = PDFExtractor(self.pdf_path)
        return extractor.extract_all()

    def build(self, extracted_data):
        builder = CSVBuilder(self.pdf_path.parent, self.poll_id)
        return builder.build_all(extracted_data)
```

---

## ğŸªµ Journalisation

Le module utilise `logging` pour tracer chaque Ã©tape :

```
INFO  [BasePipeline] ğŸ§¹ Nettoyage des anciens fichiers avant traitement...
INFO  [BasePipeline] ğŸ“¦  Extraction et construction des CSV...
ERROR [BasePipeline] Erreur inattendue dans le pipeline : <message>
```

---

## ğŸ§± Points techniques clÃ©s

| Ã‰lÃ©ment           | Description                                    |
| ----------------- | ---------------------------------------------- |
| **Pattern**       | Template Method Pattern                        |
| **Type**          | Classe abstraite (`ABC`)                       |
| **Objectif**      | Standardiser lâ€™exÃ©cution des pipelines         |
| **Validation**    | VÃ©rification stricte des entrÃ©es               |
| **ExtensibilitÃ©** | Compatible avec tout pipeline (PDF, API, etc.) |
| **Logs**          | Gestion complÃ¨te via le module `logging`       |

---
