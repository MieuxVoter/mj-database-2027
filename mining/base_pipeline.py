import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any


class BasePipeline(ABC):
    """
    Classe de base abstraite pour les pipelines d‚Äôextraction et de construction
    de donn√©es issues de sondages (Cluster17, IFOP, etc.).

    Cette classe d√©finit la structure commune, les validations initiales
    et le workflow d‚Äôex√©cution partag√© par tous les pipelines.

    Responsabilit√©s principales :
    - Valider les param√®tres d‚Äôentr√©e (PDF path, type de sondage).
    - V√©rifier la pr√©sence et la structure minimale du fichier `metadata.txt`.
    - Nettoyer les artefacts g√©n√©r√©s pr√©c√©demment (CSV, TXT).
    - Orchestrer l‚Äôex√©cution compl√®te du pipeline :
        extraction ‚Üí transformation ‚Üí g√©n√©ration des fichiers.

    Les classes d√©riv√©es DOIVENT impl√©menter :
    - `extract()` : extraction des donn√©es brutes depuis la source (PDF).
    - `build()` : transformation des donn√©es et g√©n√©ration des artefacts finaux.

    Le point d‚Äôentr√©e public du pipeline est la m√©thode `run()`, qui ex√©cute
    l‚Äôensemble du processus de mani√®re contr√¥l√©e et journalis√©e.
    """

    REQUIRED_METADATA_FIELDS = {"poll_id", "pdf_url"}

    def __init__(self, pdf_path: Path, poll_type: str):
        """
        Initialise le processus du pipeline.

        Args:
            file : Path
                Chemin absolu vers le fichier PDF √† analyser.
            poll_type : str
               Identifiant du type de sondage (ex. "pt4").
        """

        self.pdf_path: Path = pdf_path
        self.poll_type: str = poll_type
        self.logger = logging.getLogger(self.__class__.__name__)
        self._validate_inputs()

    def _validate_inputs(self) -> None:
        """
        Valide les param√®tres fournis au constructeur.

        V√©rifie que :
        - `pdf_path` est une instance de pathlib.Path et que le fichier existe.
        - `poll_type` est une cha√Æne de caract√®res valide.
        """
        if not isinstance(self.pdf_path, Path):
            self.logger.error("Le param√®tre 'pdf_path' doit √™tre une instance de pathlib.Path.")
            raise TypeError("Le param√®tre 'pdf_path' doit √™tre une instance de pathlib.Path.")
        if not self.pdf_path.exists():
            self.logger.error(f"Le fichier sp√©cifi√© est introuvable : {self.pdf_path}")
            raise FileNotFoundError(f"Le fichier sp√©cifi√© est introuvable : {self.pdf_path}")
        if not isinstance(self.poll_type, str):
            self.logger.error("Le param√®tre 'poll_type' doit √™tre une cha√Æne de caract√®res.")
            raise TypeError("Le param√®tre 'poll_type_id' doit √™tre une cha√Æne de caract√®res.")

    @abstractmethod
    def extract(self) -> tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        Extraire les donn√©es brutes depuis la source  (PDF)

        Returns:
            tuple :
                - survey_metadata (Dict[str, Any]) :
                    M√©tadonn√©es globales du sondage (taille d‚Äô√©chantillon,
                    dates, URL du PDF, etc.).
                - surveys (List[Dict[str, Any]]) :
                    Liste des tableaux de sondage extraits, accompagn√©s de
                    leurs m√©tadonn√©es sp√©cifiques.
        """
        pass

    @abstractmethod
    def build(self, survey_metadata, surveys) -> int:
        """
        Construisez les artefacts (CSV, TXT, etc.) finaux √† partir des donn√©es extraites.

        Cette m√©thode est responsable de :
        - Nettoyer et normaliser les donn√©es extraites.
        - Fusionner avec des jeux de donn√©es de r√©f√©rence si n√©cessaire.
        - G√©n√©rer les fichiers de sortie (CSV, TXT, etc.).

        """
        pass

    def _validate_metadata(self) -> None:
        """
        Valide l'existence et la structure minimale du fichier metadata.txt.

        Le fichier `metadata.txt` doit :
        - √ätre situ√© dans le m√™me r√©pertoire que le PDF.
        - Contenir des paires cl√©:valeur.
        - D√©finir tous les champs obligatoires list√©s dans `REQUIRED_METADATA_FIELDS`.
        """
        metadata_file = Path(self.pdf_path.parent) / "metadata.txt"
        if not metadata_file.is_file():
            raise FileNotFoundError(f"<< metadata.txt >> requis mais absent : {metadata_file}")
        self.logger.info("‚úÖ  << metadata.txt >> d√©tect√©")

        metadata: dict[str, str] = {}
        for line_number, raw_line in enumerate(metadata_file.read_text(encoding="utf-8").splitlines(), start=1):
            line = raw_line.strip()

            # ignorer lignes vides et commentaires
            if not line or line.startswith("#"):
                continue

            if ":" not in line:
                raise ValueError(f"Structure invalide dans metadata.txt " f"(ligne {line_number}) : '{raw_line}'")

            key, value = line.split(":", 1)
            metadata[key.strip()] = value.strip()

        missing_fields = self.REQUIRED_METADATA_FIELDS - metadata.keys()
        if missing_fields:
            raise ValueError(f"Champs obligatoires manquants dans metadata.txt : {sorted(missing_fields)}")

        self.logger.info("üìÑ  Structure de << metadata.txt >> valid√©e")

    def _cleanup_existing_files(self, extensions=("csv", "txt")) -> None:
        """
        Supprime les anciens fichiers avant le traitement si n√©cessaire.

        Tous les fichiers correspondant aux extensions fournies sont supprim√©s
        r√©cursivement dans le r√©pertoire du PDF, √† l‚Äôexception de `metadata.txt`.
        """
        try:
            base_path = self.pdf_path.parent
            files_to_delete = []

            # Rechercher tous les fichiers correspondants
            for ext in extensions:
                for f in base_path.rglob(f"*.{ext}"):
                    if f.name == "metadata.txt":
                        continue
                    files_to_delete.append(f)

            if not files_to_delete:
                self.logger.info(f"Aucun fichier .csv/.txt trouv√© √† supprimer dans : {base_path}")
                return

            for f in files_to_delete:
                try:
                    f.unlink()
                except Exception as e:
                    self.logger.error(f"Impossible de supprimer le fichier : {f} ({e})")

            self.logger.info(f"{len(files_to_delete)} ancien(s) fichier(s) supprim√©(s) dans : {base_path}")

        except Exception as e:
            self.logger.error(f"Erreur inattendue lors du nettoyage des fichiers : {e}")

    def run(self):
        """
        Ex√©cute le pipeline complet.

        √âtapes d‚Äôex√©cution :
        1. Validation du fichier `metadata.txt`.
        2. Nettoyage des fichiers de sortie existants.
        3. Extraction des donn√©es depuis la source.
        4. Construction et √©criture des artefacts finaux.

        Toute erreur rencontr√©e durant l‚Äôex√©cution est journalis√©e puis relanc√©e.
        """
        try:
            self.logger.info("üìÑ  Validation du fichier << metadata.txt >>...")
            self.logger.info("=" * 70)
            self._validate_metadata()
            self.logger.info("")

            self.logger.info("üßπ Nettoyage des anciens fichiers avant traitement...")
            self.logger.info("=" * 70)
            self._cleanup_existing_files()
            self.logger.info("")

            self.logger.info("üîç  D√©tection et extraction des pages de donn√©es... ")
            self.logger.info("=" * 70)
            survey_metadata, surveys = self.extract()
            self.logger.info("")

            self.logger.info("üì¶  Extraction et construction des CSV...")
            self.logger.info("=" * 70)
            nb_csv_created = self.build(survey_metadata, surveys)
            self.logger.info("")

            self.logger.info("=" * 70)
            self.logger.info(f"‚úÖ  {nb_csv_created} fichier(s) CSV g√©n√©r√©(s)")
            self.logger.info("")

        except FileNotFoundError as e:
            self.logger.error(f"Erreur de configuration : {e}")
            raise

        except Exception as e:
            self.logger.error(f"Erreur inattendue dans le pipeline : {e}")
            raise
