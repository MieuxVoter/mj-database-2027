import pathlib
from pathlib import Path
import logging
import re
from typing import List, Dict, Any
import pdfplumber
import pandas as pd
from datetime import date
from tabulate import tabulate
from pdfminer.layout import LTTextContainer
from pdfminer.high_level import extract_pages
from core.utils.helpers import normalize
from core.models.population import Population


class PDFExtractor:
    """
    Classe responsable de l'extraction des tableaux et des l√©gendes (captions)
    √† partir d'un PDF du barom√®tre Cluster17.
    """

    # Colonnes √† trouver dans les tableaux
    COLUMN_HEADER_PATTERNS = [
        r"vous\s+la\s+soutenez",
        r"vous\s+l['‚Äô]appreciez",
        r"vous\s+ne\s+l['‚Äô]appreciez\s*pas",
        r"vous\s+n['‚Äô]avez\s+pas\s+d['‚Äô]avis\s+sur\s+elle",
        r"vous\s+ne\s+la\s+connaissez\s+pas",
    ]

    MONTHS_FR = {
        "janvier": "01",
        "f√©vrier": "02",
        "fevrier": "02",
        "mars": "03",
        "avril": "04",
        "mai": "05",
        "juin": "06",
        "juillet": "07",
        "ao√ªt": "08",
        "aout": "08",
        "septembre": "09",
        "octobre": "10",
        "novembre": "11",
        "d√©cembre": "12",
        "decembre": "12",
    }

    def __init__(self, pdf_path: pathlib.Path) -> None:
        """
        Initialise l'extracteur PDF pour le barom√®tre Cluster17.

        Args:
            file : Path
                Chemin complet vers le fichier PDF √† analyser.
            population : Population, optionnel
                Population ou sous-√©chantillon concern√© (ex. Population.LFI)
        """
        self.pdf_path: Path = pdf_path
        self.logger = logging.getLogger(__name__)
        self._validate_inputs()

    def _validate_inputs(self) -> None:
        """
        Valide les param√®tres d'entr√©e.
        """
        if not isinstance(self.pdf_path, Path):
            self.logger.error("Le param√®tre 'pdf_path' doit √™tre une instance de pathlib.Path.")
            raise TypeError("Le param√®tre 'pdf_path' doit √™tre une instance de pathlib.Path.")
        if not self.pdf_path.exists():
            self.logger.error(f"Le fichier sp√©cifi√© est introuvable : {self.pdf_path}")
            raise FileNotFoundError(f"Le fichier sp√©cifi√© est introuvable : {self.pdf_path}")

    def _is_page_relevant(self, page_layout) -> bool:
        """
        Analysez une page PDF pour d√©terminer si elle contient un tableau correspondant
        √† une sondage du Cluster 17.

        Il repose sur la d√©tection combin√©e de plusieurs crit√®res :
        - Pr√©sence du titre attendu dans le texte normalis√©.
        - Densit√© totale de lignes suffisante (indicative de tableaux).
        - Abondance de petits blocs de texte (cellules ou lignes courtes).
        - Densit√© num√©rique √©lev√©e (valeurs en pourcentage ou num√©riques).
        - Existence d'en-t√™tes attendus dans des colonnes connues.

        Args:
            page_layout: Objet it√©rable provenant de `pdfminer`
                qui contient les √©l√©ments de texte (`LTTextContainer`)
                et leur disposition sur la page.

        Returns:
            bool: `True` si la page pr√©sente les caract√©ristiques typiques
                d'un tableau de sondage Cluster 17, `False` dans le cas contraire.
        """

        page_text = ""
        text_blocks = []

        for element in page_layout:
            if isinstance(element, LTTextContainer):
                text = element.get_text().strip()
                if text:
                    page_text += text + "\n"
                    lines = [l.strip() for l in text.split("\n") if l.strip()]
                    text_blocks.append({"text": text, "line_count": len(lines)})

        normalized_text = normalize(page_text)

        # -----------------------------------------------------------------
        # R√®gles d‚Äôidentification
        # -----------------------------------------------------------------
        # D√©tectant le titre
        has_title = bool(re.search(r"\bbarometre\b.*\bpersonnalites\b", normalized_text))
        # Densit√© totale des lignes
        total_lines = sum(b["line_count"] for b in text_blocks)
        # Blocs (petites tables)
        small_blocks = [b for b in text_blocks if 1 <= b["line_count"] <= 4]
        # Densit√© num√©rique (% ou nombre)
        num_blocks = sum(1 for b in text_blocks if "%" in b["text"] or any(ch.isdigit() for ch in b["text"]))
        has_numeric_density = num_blocks >= 5
        # R√®gles combin√©es pour tables
        has_table_structure = (
            total_lines >= 20 or len(small_blocks) >= 25  # il y a beaucoup de lignes  # beaucoup de petits blocs
        )
        has_expected_columns = sum(bool(re.search(p, normalized_text)) for p in self.COLUMN_HEADER_PATTERNS) >= 2

        return has_title and has_table_structure and has_numeric_density and has_expected_columns

    def _get_tables_population(self, page_number: int) -> List[Dict[str, Any]]:
        """
        Extrait d'une page PDF les **tableaux** et les **blocs de texte (l√©gendes ou populations)**
        qui se trouvent imm√©diatement au-dessus d'eux, en renvoyant les deux √©l√©ments dans une structure combin√©e.

        Args:
            page_number: Num√©ro de la page √† analyser (index√©e 1).
                Il doit √™tre compris entre 1 et le nombre total de pages du PDF.

        Returns:
            List[Dict[str, Any]]
                    Une liste de dictionnaires, o√π chaque √©l√©ment repr√©sente un tableau et son contexte textuel associ√©.
        """

        self.logger.debug("")
        self.logger.debug("=" * 50)
        self.logger.debug(f"Obtenir des tables et ses populations ‚Äî Page: {page_number}")
        self.logger.debug("=" * 50)
        self.logger.debug("")

        with pdfplumber.open(self.pdf_path) as pdf:

            total_pages = len(pdf.pages)

            if page_number < 1 or page_number > len(pdf.pages):
                raise ValueError(f"Num√©ro de page invalide: {page_number} / {total_pages}. ")

            page = pdf.pages[page_number - 1]

            # D√©tecter les tables
            table_objects = sorted(page.find_tables(), key=lambda t: t.bbox[1])

            if not table_objects:
                self.logger.debug(f"Aucune table d√©tect√©e √† la page {page_number}.")
                return []

            bboxes = [t.bbox for t in table_objects]

            self.logger.debug(f"Table(s) d√©tect√©e(s) :\t{len(table_objects)} ")
            self.logger.debug("")

            # Extraire tous les mots avec coordonn√©es
            words = page.extract_words(use_text_flow=True)

            y_prev_bottom = 0
            survey_data = []
            for idx, (x0, y_top, x1, y_bottom) in enumerate(bboxes, start=1):
                try:
                    self.logger.debug(f"Obtenir les information du table {idx}")
                    self.logger.debug(f"bbox table :\t({x0:.1f}, {y_top:.1f}, {x1:.1f}, {y_bottom:.1f})")

                    # Extraire texte avant la table (caption / population)
                    segment_words = [w for w in words if y_prev_bottom <= w["bottom"] <= y_top]
                    sorted_words = sorted(segment_words, key=lambda w: (w["top"], w["x0"]))
                    segment_texte = " ".join(w["text"] for w in sorted_words)

                    # supprimer le titre principal
                    clean_text = re.sub(
                        r"BAROM√àTRE DES PERSONNALIT√âS\s+[A-Z√â√à√ä√é√î√õ√Ç√Ä√ô√á\-]+", "", segment_texte, flags=re.IGNORECASE
                    ).strip()

                    population = None
                    population_label = None
                    if clean_text:
                        self.logger.debug(f"L√©gende:\t{clean_text}")
                        population_detected = Population.detect_from_text(clean_text)
                        if population_detected:
                            population, population_label = population_detected
                            self.logger.debug(f"population:\t{population}")

                    # Extraire la table
                    df = pd.DataFrame(table_objects[idx - 1].extract())

                    # Nettoyage du DataFrame
                    df = df.dropna(how="all").reset_index(drop=True)
                    if not df.empty:
                        df.columns = df.iloc[0]
                        df = df[1:].reset_index(drop=True)

                    self.logger.debug(f"columns: {df.columns.tolist()}")
                    self.logger.debug("Aper√ßu du DataFrame :\n" + tabulate(df.head(), headers="keys", tablefmt="psql"))

                    survey_data.append(
                        {
                            "Page": page_number,
                            "Table id": idx,
                            "L√©gende de tableau": clean_text,
                            "Population": population,
                            "√âtiquette de population": population_label,
                            "df": df,
                        }
                    )

                    y_prev_bottom = y_bottom
                    self.logger.debug("")
                except (KeyError, IndexError, ValueError) as e:
                    self.logger.warning(f"Table ignor√©e | page={page_number} | table={idx} | reason={e}")

        return survey_data

    def _read_metadata_txt(self) -> Dict[str, str]:
        """
        Lire un fichier metadata.txt format√© sous forme de paires ¬´ cl√© : valeur ¬ª.

        Args:
            path : Path
                Chemin d'acc√®s au fichier metadata.txt.

        Returns:
            Dict[str, str]
                Dictionnaire contenant des cl√©s et des valeurs de m√©tadonn√©es.

        """
        metadata_path = self.pdf_path.parent / "metadata.txt"

        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

        metadata: Dict[str, str] = {}

        with metadata_path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                if ":" not in line:
                    raise ValueError(f"Malformed metadata line: {line}")

                key, value = line.split(":", 1)
                metadata[key.strip()] = value.strip()

        return metadata

    def _extract_methodology_metadata(self, end_page: int = 5) -> Dict[str, Any]:
        """
        Extrait les m√©tadonn√©es m√©thodologiques cl√©s √† partir de la section
        ¬´ M√âTHODOLOGIE ¬ª d‚Äôun PDF Cluster17.

        Cette m√©thode parcourt les premi√®res pages du document afin de localiser
        la page contenant le titre ¬´ M√âTHODOLOGIE ¬ª, puis en extrait les informations
        suivantes :
        - la taille de l‚Äô√©chantillon (nombre de personnes interrog√©es),
        - les dates de r√©alisation des interviews (format ISO YYYY-MM-DD).

        Args:
            end_page (int, optional):
                Nombre maximum de pages √† analyser depuis le d√©but du PDF.
                Par d√©faut √† 5, ce qui couvre g√©n√©ralement la section m√©thodologique.

        Returns:
            Dict[str, Any]:
                Dictionnaire contenant les m√©tadonn√©es extraites :
                {
                    "sample_size": int,        # Taille de l‚Äô√©chantillon
                    "start_date": str,         # Date de d√©but des interviews (YYYY-MM-DD)
                    "end_date": str,           # Date de fin des interviews (YYYY-MM-DD)
                }
        """

        methodology_text = ""

        # Trouver la page "M√âTHODOLOGIE"
        with pdfplumber.open(self.pdf_path) as pdf:
            for idx, page in enumerate(pdf.pages[:end_page], start=1):
                page_text = page.extract_text()
                if not page_text:
                    continue

                if re.search(r"\bm[√©e]thodologie\b", page_text, flags=re.IGNORECASE):
                    methodology_text = page_text
                    self.logger.info(f"üìê  Page M√âTHODOLOGIE d√©tect√©e (page {idx})")
                    break

        if not methodology_text:
            raise ValueError("Page M√âTHODOLOGIE introuvable dans le PDF")

        # -------------------------
        # Taille de l‚Äô√©chantillon
        # -------------------------
        sample_match = re.search(
            r"[√©e]chantillon\s+(?:de|:)\s*([\d\s]+)\s+personnes", methodology_text, flags=re.IGNORECASE
        )
        if not sample_match:
            raise ValueError("Impossible d‚Äôextraire la taille de l‚Äô√©chantillon")

        sample_size = int(sample_match.group(1).replace(" ", ""))
        self.logger.debug(f"sample_size: {sample_size}")

        # -------------------------
        # Dates d‚Äôinterviews
        # -------------------------
        RE_ONE_MONTH = re.compile(
            r"Interviews r√©alis√©es du\s+(\d{1,2})\s+au\s+(\d{1,2})\s+" r"([a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß]+)\s+(\d{4})",
            flags=re.IGNORECASE,
        )

        RE_TWO_MONTHS = re.compile(
            r"Interviews r√©alis√©es du\s+(\d{1,2})\s+"
            r"([a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß]+)\s+au\s+"
            r"(\d{1,2})(?:er)?\s+"
            r"([a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß]+)\s+(\d{2,4})",
            flags=re.IGNORECASE,
        )

        # Cas A : un seul mois (ex: octobre 2025)
        m = RE_ONE_MONTH.search(methodology_text)
        if m:
            d1, d2, month, year = m.groups()

            month_norm = month.lower()
            if month_norm not in self.MONTHS_FR:
                raise ValueError(f"Mois non reconnu : {month}")

            y = int(year)
            m_num = int(self.MONTHS_FR[month_norm])

            start_date = date(y, m_num, int(d1)).isoformat()
            end_date = date(y, m_num, int(d2)).isoformat()

        # Cas B : deux mois (ex: ao√ªt ‚Üí septembre 25)
        else:
            m = RE_TWO_MONTHS.search(methodology_text)
            if not m:
                raise ValueError("Impossible d‚Äôextraire les dates d‚Äôinterviews")

            d1, m1, d2, m2, year = m.groups()

            m1 = m1.lower()
            m2 = m2.lower()

            if m1 not in self.MONTHS_FR or m2 not in self.MONTHS_FR:
                raise ValueError(f"Mois non reconnu : {m1}, {m2}")

            y = int(year) if len(year) == 4 else int(f"20{year}")

            start_date = date(y, int(self.MONTHS_FR[m1]), int(d1)).isoformat()
            end_date = date(y, int(self.MONTHS_FR[m2]), int(d2)).isoformat()

        self.logger.debug(f"start_date: {start_date} | end_date: {end_date}")
        self.logger.debug("")

        # -------------------------
        # Lecture de l'URL du pdf √† partir de metadata.txt
        # -------------------------
        metadata_txt = self._read_metadata_txt()
        pdf_url = metadata_txt.get("pdf_url")
        if not pdf_url:
            raise ValueError("pdf_url introuvable dans metadata.txt")

        self.logger.debug(f"pdf_url: {pdf_url}")

        return {"sample_size": sample_size, "start_date": start_date, "end_date": end_date, "pdf_url": pdf_url}

    def extract_all(self) -> tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        Ex√©cute l'extraction compl√®te du fichier PDF :
        - D√©tection des pages pertinentes
        - Extraction des tableaux et populations associ√©es

        Returns:
            Dict[str, Any]:
                Dictionnaire contenant les m√©tadonn√©es :
                {
                    "sample_size": int,        # Taille de l‚Äô√©chantillon
                    "start_date": str,         # Date de d√©but des interviews (YYYY-MM-DD)
                    "end_date": str,           # Date de fin des interviews (YYYY-MM-DD)
                }

            List[Dict[str, Any]]
                    Une liste de dictionnaires, o√π chaque √©l√©ment repr√©sente un tableau et son contexte textuel associ√©.
        """
        # -----------------------------------------------------------------
        # Extraction des m√©tadonn√©es de l'enqu√™te
        # -----------------------------------------------------------------
        survey_metadata = self._extract_methodology_metadata()

        # -----------------------------------------------------------------
        # D√©tection des pages pertinentes contenant des sondages
        # -----------------------------------------------------------------
        pages = list(extract_pages(str(self.pdf_path)))
        total_pages = len(pages)
        data_pages: List[int] = []

        for page_num in range(1, total_pages + 1):
            page_layout = pages[page_num - 1]
            if self._is_page_relevant(page_layout):
                data_pages.append(page_num)

        if not data_pages:
            self.logger.warning("Aucune page pertinente d√©tect√©e dans ce PDF")
            return []

        self.logger.info(f"üìä  {len(data_pages)} page(s) de donn√©es d√©tect√©e(s) :")

        # -----------------------------------------------------------------
        # Obtenir les tableaux et les populations
        # -----------------------------------------------------------------
        surveys: List[Dict[str, Any]] = []
        for page_number in data_pages:
            survey_data = self._get_tables_population(page_number)
            for table in survey_data:
                self.logger.info(f"  ‚Ä¢ Page {page_number} : {table['√âtiquette de population']}")
            surveys.extend(survey_data)

        if not surveys:
            self.logger.warning("Aucune table extraite du PDF")
            return []

        return survey_metadata, surveys
